[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISS608-VAA-GroupProj",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Proposal/Proposal.html",
    "href": "Proposal/Proposal.html",
    "title": "Here in Loompa land, it’s both luscious and green",
    "section": "",
    "text": "In this hands-on exercise, you will gain hands-on experience on using:\n\nggstatsplot package to create visual graphics with rich statistical information,\nperformance package to visualise model diagnostics, and\nparameters package to visualise model parameters\n\n\n\n\n\nggstatsplot is an extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves.\n\nTo provide alternative statistical inference methods by default.\nTo follow best practices for statistical reporting. For all statistical tests reported in the plots, the default template abides by the APA gold standard for statistical reporting.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nparameter refers to the degree of freedom\nAn effect size of 0.77 is a standardized measure of the magnitude of a treatment or intervention effect, or the strength of an association between two variables. Guideline is that an effect size of 0.2 is considered small, 0.5 is considered moderate, and 0.8 is considered large.\nCI of 95% means if we replicate our sampling from underlying distribution many times, 95% of our samples will have their means within this interval.\n\n\n\n\n\n\n\n\nIn this exercise, ggstatsplot and tidyverse will be used.\n\npacman::p_load(ggstatsplot, tidyverse,nortest, ggdist)\n\n\n\n\nLets import the Exam_data.csv using the read_xls() function.\n\nexam &lt;- read_csv('data/Exam_data.csv')\n\nTake a glimpse at the data.\n\nexam\n\n# A tibble: 322 × 7\n   ID         CLASS GENDER RACE    ENGLISH MATHS SCIENCE\n   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 Student321 3I    Male   Malay        21     9      15\n 2 Student305 3I    Female Malay        24    22      16\n 3 Student289 3H    Male   Chinese      26    16      16\n 4 Student227 3F    Male   Chinese      27    77      31\n 5 Student318 3I    Male   Malay        27    11      25\n 6 Student306 3I    Female Malay        31    16      16\n 7 Student313 3I    Male   Chinese      31    21      25\n 8 Student316 3I    Male   Malay        31    18      27\n 9 Student312 3I    Male   Malay        33    19      15\n10 Student297 3H    Male   Indian       34    49      37\n# ℹ 312 more rows\n\n\n\nglimpse(exam)\n\nRows: 322\nColumns: 7\n$ ID      &lt;chr&gt; \"Student321\", \"Student305\", \"Student289\", \"Student227\", \"Stude…\n$ CLASS   &lt;chr&gt; \"3I\", \"3I\", \"3H\", \"3F\", \"3I\", \"3I\", \"3I\", \"3I\", \"3I\", \"3H\", \"3…\n$ GENDER  &lt;chr&gt; \"Male\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"M…\n$ RACE    &lt;chr&gt; \"Malay\", \"Malay\", \"Chinese\", \"Chinese\", \"Malay\", \"Malay\", \"Chi…\n$ ENGLISH &lt;dbl&gt; 21, 24, 26, 27, 27, 31, 31, 31, 33, 34, 34, 36, 36, 36, 37, 38…\n$ MATHS   &lt;dbl&gt; 9, 22, 16, 77, 11, 16, 21, 18, 19, 49, 39, 35, 23, 36, 49, 30,…\n$ SCIENCE &lt;dbl&gt; 15, 16, 16, 31, 25, 16, 25, 27, 15, 37, 42, 22, 32, 36, 35, 45…\n\n\n\n\n\nIn the code chunk below, gghistostats() is used to to build an visual of one-sample test on English scores.\n\nA one-sample test is a statistical hypothesis test used to determine whether the mean of a single sample of data differs significantly from a known or hypothesized value.\nIt is a statistical test that compares the mean of a sample to a specified value, such as a population mean, to see if there is enough evidence to reject the null hypothesis that the sample comes from a population with the specified mean.\n\nH0: EL average score is 60.\n\nset.seed(1234)  #&lt;&lt;&lt; important to set if we use bayes statistics\n\ngghistostats(data=exam,\n             x = ENGLISH,\n             type='bayes',  #&lt;&lt; '\n             test.value =60,\n             xlab = 'English scores')\n\n\n\n\n\n\n\n\nA Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\nThat’s because the Bayes factor gives us a way to evaluate the data in favor of a null hypothesis, and to use external information to do so. It tells us what the weight of the evidence is in favor of a given hypothesis.\nWhen we are comparing two hypotheses, H1 (the alternate hypothesis) and H0 (the null hypothesis), the Bayes Factor is often written as B10. It can be defined mathematically as\nThe Schwarz criterion is one of the easiest ways to calculate rough approximation of the Bayes Factor.\n\n\n\n\nA Bayes Factor can be any positive number. One of the most common interpretations is this one—first proposed by Harold Jeffereys (1961) and slightly modified by Lee and Wagenmakers in 2013:\n\n\nReference website from r-bloggers\nThe one-sample Wilcoxon test (non parametric) will tell us whether the scores are significantly different from 60 or not (and thus whether they are different from 60 in the population or not)\nH0: EL scores = 60\nH1: EL scores != 60\nThe scores are assumed to be independent (a student’s score is not impacted or influenced by the score of another student)\n\nwilcox.test(exam$ENGLISH,\n            mu = 60)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  exam$ENGLISH\nV = 38743, p-value = 3.435e-16\nalternative hypothesis: true location is not equal to 60\n\n\nInterpretation\nP-value&lt;0.05, we have enough statistical evidence to reject the null hypothesis and conclude that the EL scores are significantly different from 60.\n\n\n\n\n\n\nNote\n\n\n\nBy default, it is a two-tailed test that is done. As for the t.test() function, we can specify that a one-sided test is required by using either the alternative = “greater” or alternative = “less argument in the wilcox.test() function.\n\n\nCombine statistical test and plot\n\nset.seed(1234)\n\ngghistostats(data=exam,\n             x = ENGLISH,\n             type='nonparametric', #nonparametric (median) = Wilcoxon, parametric = t-test (default is look for mean and unequal variance method)\n             test.value =60,\n             conf.level = 0.95,\n             xlab = 'English scores')\n\n\n\n\nDid we forget to check if English scores follow a normal distribution? Use ad.test from nortest library.\nH0: EL scores follows normal distribution\nH1: EL scores do not follow normal distribution.\n\nad.test(exam$ENGLISH)\n\n\n    Anderson-Darling normality test\n\ndata:  exam$ENGLISH\nA = 4.3661, p-value = 7.341e-11\n\n\nResults from the Anderson_darling normality test shows enough statistical evidence to reject the null hypothesis and conclude that the EL scores do not follow normal distribution . Thus the use of non parametric test is correct.\n\n\n\n\n\n\nOn Parametric and Non-parametric types\n\n\n\ntype= parametric: default look for mean and assumes unequal variance method\ntype = Non parametric: student-t test and use median (not mean!!)\n\n\n\n\n\n\nIn the code chunk below, ggbetweenstats() is used to build a visual for two-sample mean test of Maths scores by gender (independent).\nH0: Mean of F and M Math scores are the same.\nH1: Mean of F and M Math scores are not the same.\n\nggbetweenstats(data=exam,\n               x=GENDER,\n               y=MATHS,\n               type='np',\n               messages=FALSE)\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\nSince p-value &gt; 0.05, we do not have enough statistical evidence to reject the null hypothesis that mean of Math scores of both gender are the same.\nHowever, if we check for normality of Math scores of each gender.\n\n# perform Shapiro-Wilk test on math scores by gender\nshapiro_test &lt;- by(exam$MATHS, exam$GENDER, shapiro.test)\n\n# extract p-values\np_values &lt;- sapply(shapiro_test, function(x) x$p.value)\n# print results\nprint(p_values)\n\n      Female         Male \n1.603536e-07 6.268520e-08 \n\n\n\n\n\n\n\n\nNote\n\n\n\nThe by() function is used to apply a function to subsets of a data frame or vector split by one or more factors. In the above code, we use by() to split the math_score column by gender, and apply the shapiro.test() function to each group.\n\n\nH0: Math scores by gender follows normal distribution.\nH1: Math scores by gender do not follow normal distribution.\nFrom the Shapiro-Wilk test results, we have enough statistical evidence to reject the null hypothesis and conclude that the Math scores by gender does not follow a normal distribution. Thus the use of ‘np’ is appropriate.\n\n\n\nIn the code chunk below, ggbetweenstats() is used to build a visual for One-way ANOVA test on English score by race (Independent 4 sample mean).\n\nggbetweenstats(\n  data = exam,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci=TRUE,\n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",  # 'ns': shows only non-sig, 's': shows only sig, 'all': both \n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n## might need to call library(PMCMRplus) and library(rstantools) if this code chunck doesnt work.\n\nSince p-value &lt; 0.05, we have enough statistical evidence to reject the null hypothesis and conclude that NOT ALL means of EL scores by race are the same. The results shows that the means of EL scores of Chinese, Indian and Malay are significantly different.\nOnce again, lets go backwards and confirm that the distribution of EL scores by RACE conforms to normal distribution.\n\n# perform Shapiro-Wilk test on math scores by gender\nshapiro_test &lt;- by(exam$ENGLISH, exam$RACE, shapiro.test)\n\n# extract p-values\np_values &lt;- sapply(shapiro_test, function(x) x$p.value)\n# print results\nprint(p_values)\n\n     Chinese       Indian        Malay       Others \n1.305153e-07 8.482600e-01 1.251020e-02 5.181740e-01 \n\n\nH0: EL scores by Race follow normal distribution. H1: EL scores by Race do not follow normal distribution.\nThe results of the Shapiro-wilk test shows p_value of all EL score distribution by race follows normal distribution.\n\n\n\ntype argument entered by us will determine the centrality tendency measure displayed\n\n\nmean for parametric statistics\nmedian for non-parametric statistics\ntrimmed mean for robust statistics\nMAP estimator for Bayesian statistics\n\n\n\n\n\nEarlier, we have checked that EL scores do not follow a normal distribution. Now we will do the same for Math scores.\n\nad.test(exam$MATHS)\n\n\n    Anderson-Darling normality test\n\ndata:  exam$MATHS\nA = 7.9125, p-value &lt; 2.2e-16\n\n\nSince the p-value &lt; 0.05, we have enough statistical evidence to reject the null hypothesis and conclude that the Math scores also do not follow normal distribution.\nIn the code chunk below, ggscatterstats() is used to build a visual for Significant Test of Correlation between Maths scores and English scores.\n\nggscatterstats(\n  data = exam,\n  x = MATHS,\n  y = ENGLISH,\n  type='nonparametric', # 'parametric', 'robust', 'bayes'\n  marginal = FALSE,\n  )\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nI have chosen a non parametric version of this test as both Math and EL scores do not follow normal distribution.\n\n\n\n\n\nIn the code chunk below, the Maths scores is binned into a 4-class variable by using cut().\nWe will create a new dataframe exam1 similar to exam df but with extra column called ‘MATHS_bins’.\n\nexam1 &lt;- exam %&gt;% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0,60,75,85,100))\n)\n\n\nexam1\n\n# A tibble: 322 × 8\n   ID         CLASS GENDER RACE    ENGLISH MATHS SCIENCE MATHS_bins\n   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;     \n 1 Student321 3I    Male   Malay        21     9      15 (0,60]    \n 2 Student305 3I    Female Malay        24    22      16 (0,60]    \n 3 Student289 3H    Male   Chinese      26    16      16 (0,60]    \n 4 Student227 3F    Male   Chinese      27    77      31 (75,85]   \n 5 Student318 3I    Male   Malay        27    11      25 (0,60]    \n 6 Student306 3I    Female Malay        31    16      16 (0,60]    \n 7 Student313 3I    Male   Chinese      31    21      25 (0,60]    \n 8 Student316 3I    Male   Malay        31    18      27 (0,60]    \n 9 Student312 3I    Male   Malay        33    19      15 (0,60]    \n10 Student297 3H    Male   Indian       34    49      37 (0,60]    \n# ℹ 312 more rows\n\n\nIn this code chunk below ggbarstats() is used to build a visual for Significant Test of Association.\n(Two categorical variables) H0: There is no association between mathbin and gender.\nH1: There is an association between mathbin and gender.\n\nggbarstats(exam1,\n            x=MATHS_bins,\n            y=GENDER)\n\n\n\n\nFrom the results above , p-value &gt; 0.05 thus we have not enough statistical evidence to reject the null hypothesis that there is not association between the mathbin and gender variables.\n\n\n\n\nIn this section, I will learn how to visualise model diagnostic and model parameters by using parameters package.\n\nToyota Corolla case study will be used. The purpose of study is to build a model to discover factors affecting prices of used-cars by taking into consideration a set of explanatory variables.\n\n\n\n\n\n\n\n\npacman::p_load(readxl, performance, parameters, see)\n\n\n\nIn the code chunk below, read_xls() of readxl package is used to import the data worksheet of ToyotaCorolla.xls workbook into R.\n\ncar_resale &lt;- read_xls('data/ToyotaCorolla.xls',\n                       sheet='data')\n\n\nglimpse(car_resale)\n\nRows: 1,436\nColumns: 38\n$ Id               &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16…\n$ Model            &lt;chr&gt; \"TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\", \"TOY…\n$ Price            &lt;dbl&gt; 13500, 13750, 13950, 14950, 13750, 12950, 16900, 1860…\n$ Age_08_04        &lt;dbl&gt; 23, 23, 24, 26, 30, 32, 27, 30, 27, 23, 25, 22, 25, 3…\n$ Mfg_Month        &lt;dbl&gt; 10, 10, 9, 7, 3, 1, 6, 3, 6, 10, 8, 11, 8, 2, 1, 5, 3…\n$ Mfg_Year         &lt;dbl&gt; 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002,…\n$ KM               &lt;dbl&gt; 46986, 72937, 41711, 48000, 38500, 61000, 94612, 7588…\n$ Fuel_Type        &lt;chr&gt; \"Diesel\", \"Diesel\", \"Diesel\", \"Diesel\", \"Diesel\", \"Di…\n$ HP               &lt;dbl&gt; 90, 90, 90, 90, 90, 90, 90, 90, 192, 69, 192, 192, 19…\n$ Met_Color        &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,…\n$ Color            &lt;chr&gt; \"Blue\", \"Silver\", \"Blue\", \"Black\", \"Black\", \"White\", …\n$ Automatic        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ CC               &lt;dbl&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 1800,…\n$ Doors            &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,…\n$ Cylinders        &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,…\n$ Gears            &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 5, 5,…\n$ Quarterly_Tax    &lt;dbl&gt; 210, 210, 210, 210, 210, 210, 210, 210, 100, 185, 100…\n$ Weight           &lt;dbl&gt; 1165, 1165, 1165, 1165, 1170, 1170, 1245, 1245, 1185,…\n$ Mfr_Guarantee    &lt;dbl&gt; 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,…\n$ BOVAG_Guarantee  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,…\n$ Guarantee_Period &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 12, 3, 3, 3, 3, 3, 3, 1…\n$ ABS              &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airbag_1         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airbag_2         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,…\n$ Airco            &lt;dbl&gt; 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Automatic_airco  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,…\n$ Boardcomputer    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,…\n$ CD_Player        &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,…\n$ Central_Lock     &lt;dbl&gt; 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Powered_Windows  &lt;dbl&gt; 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Power_Steering   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Radio            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,…\n$ Mistlamps        &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,…\n$ Sport_Model      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,…\n$ Backseat_Divider &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,…\n$ Metallic_Rim     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,…\n$ Radio_cassette   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,…\n$ Tow_Bar          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,…\n\n\nNotice that the output object car_resale is a tibble data frame.\n\n\n\nThe code chunk below is used to calibrate a multiple linear regression model by using lm() of Base Stats of R.\n\nmodel &lt;- lm(Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period,\n            data=car_resale)\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\n\n\nIn the code chunk, check_collinearity() of performance package.\n\ncheck_collinearity(model)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\ncheck_c &lt;- check_collinearity(model)\nplot(check_c)\n\n\n\n\nWe can see high collinearity between Age and Mfg_Year. One is derived from the other. We should remove one of them and repeat muliti collinearity check again for the new model.\n\n\n\nIn the code chunk, check_normality() of performance package.\nNotice that the Mfg_Year variable has been removed from the independent variables list.\n\nmodel1 &lt;- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\n\ncheck_c1 &lt;- check_collinearity(model1)\nplot(check_c1)\n\n\n\n\n\ncheck_n &lt;- check_normality(model1)\n\n\nplot(check_n)\n\n\n\n\n\n\n\n\n\n\nRecap: Assumptions of linear regression\n\n\n\nIn linear regression, one of the key assumptions is that the residuals (the differences between the predicted values and the actual values) are normally distributed. The normality assumption is important because it affects the validity of statistical inference procedures such as hypothesis testing and confidence intervals.\nIf the residuals are not normally distributed, it may indicate that the linear regression model is not a good fit for the data and that alternative modeling approaches may be needed.\n\n\n\n\n\nIn the code chunk, check_heteroscedasticity() of performance package.\nHeteroscedasticity refers to a situation where the variance of the errors (or residuals) in the linear regression model is not constant across different levels of the predictor variable(s).\nIf heteroscedasticity is detected, there are several ways to address it, including transforming the data, using weighted least squares regression, or using robust standard errors. In DAl, we rebuild another model by creating subclasses out of the original Y variable.\n\ncheck_h &lt;- check_heteroscedasticity(model1)\n\n\nplot(check_h)\n\n\n\n\nFrom the graph above, there is a slight sign of heteroscedasticity as the residuals seem to be funnelled outwards as the fitted values increase.\n\n\n\nWe can also perform the complete check by using check_model().\n\ncheck_model(model1)\n\n\n\n\n\n\n\nIn the code below, plot() of see package and parameters() of parameters package is used to visualise the parameters of a regression model.\n\nplot(parameters(model1))\n\n\n\n\n\n\n\nIn the code below, ggcoefstats() of ggstatsplot package to visualise the parameters of a regression model.\n\nggcoefstats(model1, \n            output = \"plot\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that there are five types of callouts, including: note, warning, important, tip, and caution.\n\n\n\n\n\n\n\n\nTip with Title\n\n\n\nThis is an example of a callout with a title.\n\n\n\n\n\n\n\n\nExpand To Learn About Collapse\n\n\n\n\n\nThis is an example of a ‘folded’ caution callout that can be expanded by the user. You can use collapse=\"true\" to collapse it by default or collapse=\"false\" to make a collapsible callout that is expanded by default."
  },
  {
    "objectID": "Proposal/Proposal.html#learning-outcome",
    "href": "Proposal/Proposal.html#learning-outcome",
    "title": "Here in Loompa land, it’s both luscious and green",
    "section": "",
    "text": "In this hands-on exercise, you will gain hands-on experience on using:\n\nggstatsplot package to create visual graphics with rich statistical information,\nperformance package to visualise model diagnostics, and\nparameters package to visualise model parameters"
  },
  {
    "objectID": "Proposal/Proposal.html#visual-statistical-analysis-with-ggstatsplot",
    "href": "Proposal/Proposal.html#visual-statistical-analysis-with-ggstatsplot",
    "title": "Here in Loompa land, it’s both luscious and green",
    "section": "",
    "text": "ggstatsplot is an extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves.\n\nTo provide alternative statistical inference methods by default.\nTo follow best practices for statistical reporting. For all statistical tests reported in the plots, the default template abides by the APA gold standard for statistical reporting.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nparameter refers to the degree of freedom\nAn effect size of 0.77 is a standardized measure of the magnitude of a treatment or intervention effect, or the strength of an association between two variables. Guideline is that an effect size of 0.2 is considered small, 0.5 is considered moderate, and 0.8 is considered large.\nCI of 95% means if we replicate our sampling from underlying distribution many times, 95% of our samples will have their means within this interval."
  },
  {
    "objectID": "Proposal/Proposal.html#getting-started",
    "href": "Proposal/Proposal.html#getting-started",
    "title": "Here in Loompa land, it’s both luscious and green",
    "section": "",
    "text": "In this exercise, ggstatsplot and tidyverse will be used.\n\npacman::p_load(ggstatsplot, tidyverse,nortest, ggdist)\n\n\n\n\nLets import the Exam_data.csv using the read_xls() function.\n\nexam &lt;- read_csv('data/Exam_data.csv')\n\nTake a glimpse at the data.\n\nexam\n\n# A tibble: 322 × 7\n   ID         CLASS GENDER RACE    ENGLISH MATHS SCIENCE\n   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 Student321 3I    Male   Malay        21     9      15\n 2 Student305 3I    Female Malay        24    22      16\n 3 Student289 3H    Male   Chinese      26    16      16\n 4 Student227 3F    Male   Chinese      27    77      31\n 5 Student318 3I    Male   Malay        27    11      25\n 6 Student306 3I    Female Malay        31    16      16\n 7 Student313 3I    Male   Chinese      31    21      25\n 8 Student316 3I    Male   Malay        31    18      27\n 9 Student312 3I    Male   Malay        33    19      15\n10 Student297 3H    Male   Indian       34    49      37\n# ℹ 312 more rows\n\n\n\nglimpse(exam)\n\nRows: 322\nColumns: 7\n$ ID      &lt;chr&gt; \"Student321\", \"Student305\", \"Student289\", \"Student227\", \"Stude…\n$ CLASS   &lt;chr&gt; \"3I\", \"3I\", \"3H\", \"3F\", \"3I\", \"3I\", \"3I\", \"3I\", \"3I\", \"3H\", \"3…\n$ GENDER  &lt;chr&gt; \"Male\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"M…\n$ RACE    &lt;chr&gt; \"Malay\", \"Malay\", \"Chinese\", \"Chinese\", \"Malay\", \"Malay\", \"Chi…\n$ ENGLISH &lt;dbl&gt; 21, 24, 26, 27, 27, 31, 31, 31, 33, 34, 34, 36, 36, 36, 37, 38…\n$ MATHS   &lt;dbl&gt; 9, 22, 16, 77, 11, 16, 21, 18, 19, 49, 39, 35, 23, 36, 49, 30,…\n$ SCIENCE &lt;dbl&gt; 15, 16, 16, 31, 25, 16, 25, 27, 15, 37, 42, 22, 32, 36, 35, 45…\n\n\n\n\n\nIn the code chunk below, gghistostats() is used to to build an visual of one-sample test on English scores.\n\nA one-sample test is a statistical hypothesis test used to determine whether the mean of a single sample of data differs significantly from a known or hypothesized value.\nIt is a statistical test that compares the mean of a sample to a specified value, such as a population mean, to see if there is enough evidence to reject the null hypothesis that the sample comes from a population with the specified mean.\n\nH0: EL average score is 60.\n\nset.seed(1234)  #&lt;&lt;&lt; important to set if we use bayes statistics\n\ngghistostats(data=exam,\n             x = ENGLISH,\n             type='bayes',  #&lt;&lt; '\n             test.value =60,\n             xlab = 'English scores')\n\n\n\n\n\n\n\n\nA Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\nThat’s because the Bayes factor gives us a way to evaluate the data in favor of a null hypothesis, and to use external information to do so. It tells us what the weight of the evidence is in favor of a given hypothesis.\nWhen we are comparing two hypotheses, H1 (the alternate hypothesis) and H0 (the null hypothesis), the Bayes Factor is often written as B10. It can be defined mathematically as\nThe Schwarz criterion is one of the easiest ways to calculate rough approximation of the Bayes Factor.\n\n\n\n\nA Bayes Factor can be any positive number. One of the most common interpretations is this one—first proposed by Harold Jeffereys (1961) and slightly modified by Lee and Wagenmakers in 2013:\n\n\nReference website from r-bloggers\nThe one-sample Wilcoxon test (non parametric) will tell us whether the scores are significantly different from 60 or not (and thus whether they are different from 60 in the population or not)\nH0: EL scores = 60\nH1: EL scores != 60\nThe scores are assumed to be independent (a student’s score is not impacted or influenced by the score of another student)\n\nwilcox.test(exam$ENGLISH,\n            mu = 60)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  exam$ENGLISH\nV = 38743, p-value = 3.435e-16\nalternative hypothesis: true location is not equal to 60\n\n\nInterpretation\nP-value&lt;0.05, we have enough statistical evidence to reject the null hypothesis and conclude that the EL scores are significantly different from 60.\n\n\n\n\n\n\nNote\n\n\n\nBy default, it is a two-tailed test that is done. As for the t.test() function, we can specify that a one-sided test is required by using either the alternative = “greater” or alternative = “less argument in the wilcox.test() function.\n\n\nCombine statistical test and plot\n\nset.seed(1234)\n\ngghistostats(data=exam,\n             x = ENGLISH,\n             type='nonparametric', #nonparametric (median) = Wilcoxon, parametric = t-test (default is look for mean and unequal variance method)\n             test.value =60,\n             conf.level = 0.95,\n             xlab = 'English scores')\n\n\n\n\nDid we forget to check if English scores follow a normal distribution? Use ad.test from nortest library.\nH0: EL scores follows normal distribution\nH1: EL scores do not follow normal distribution.\n\nad.test(exam$ENGLISH)\n\n\n    Anderson-Darling normality test\n\ndata:  exam$ENGLISH\nA = 4.3661, p-value = 7.341e-11\n\n\nResults from the Anderson_darling normality test shows enough statistical evidence to reject the null hypothesis and conclude that the EL scores do not follow normal distribution . Thus the use of non parametric test is correct.\n\n\n\n\n\n\nOn Parametric and Non-parametric types\n\n\n\ntype= parametric: default look for mean and assumes unequal variance method\ntype = Non parametric: student-t test and use median (not mean!!)\n\n\n\n\n\n\nIn the code chunk below, ggbetweenstats() is used to build a visual for two-sample mean test of Maths scores by gender (independent).\nH0: Mean of F and M Math scores are the same.\nH1: Mean of F and M Math scores are not the same.\n\nggbetweenstats(data=exam,\n               x=GENDER,\n               y=MATHS,\n               type='np',\n               messages=FALSE)\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\nSince p-value &gt; 0.05, we do not have enough statistical evidence to reject the null hypothesis that mean of Math scores of both gender are the same.\nHowever, if we check for normality of Math scores of each gender.\n\n# perform Shapiro-Wilk test on math scores by gender\nshapiro_test &lt;- by(exam$MATHS, exam$GENDER, shapiro.test)\n\n# extract p-values\np_values &lt;- sapply(shapiro_test, function(x) x$p.value)\n# print results\nprint(p_values)\n\n      Female         Male \n1.603536e-07 6.268520e-08 \n\n\n\n\n\n\n\n\nNote\n\n\n\nThe by() function is used to apply a function to subsets of a data frame or vector split by one or more factors. In the above code, we use by() to split the math_score column by gender, and apply the shapiro.test() function to each group.\n\n\nH0: Math scores by gender follows normal distribution.\nH1: Math scores by gender do not follow normal distribution.\nFrom the Shapiro-Wilk test results, we have enough statistical evidence to reject the null hypothesis and conclude that the Math scores by gender does not follow a normal distribution. Thus the use of ‘np’ is appropriate.\n\n\n\nIn the code chunk below, ggbetweenstats() is used to build a visual for One-way ANOVA test on English score by race (Independent 4 sample mean).\n\nggbetweenstats(\n  data = exam,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci=TRUE,\n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",  # 'ns': shows only non-sig, 's': shows only sig, 'all': both \n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n## might need to call library(PMCMRplus) and library(rstantools) if this code chunck doesnt work.\n\nSince p-value &lt; 0.05, we have enough statistical evidence to reject the null hypothesis and conclude that NOT ALL means of EL scores by race are the same. The results shows that the means of EL scores of Chinese, Indian and Malay are significantly different.\nOnce again, lets go backwards and confirm that the distribution of EL scores by RACE conforms to normal distribution.\n\n# perform Shapiro-Wilk test on math scores by gender\nshapiro_test &lt;- by(exam$ENGLISH, exam$RACE, shapiro.test)\n\n# extract p-values\np_values &lt;- sapply(shapiro_test, function(x) x$p.value)\n# print results\nprint(p_values)\n\n     Chinese       Indian        Malay       Others \n1.305153e-07 8.482600e-01 1.251020e-02 5.181740e-01 \n\n\nH0: EL scores by Race follow normal distribution. H1: EL scores by Race do not follow normal distribution.\nThe results of the Shapiro-wilk test shows p_value of all EL score distribution by race follows normal distribution.\n\n\n\ntype argument entered by us will determine the centrality tendency measure displayed\n\n\nmean for parametric statistics\nmedian for non-parametric statistics\ntrimmed mean for robust statistics\nMAP estimator for Bayesian statistics\n\n\n\n\n\nEarlier, we have checked that EL scores do not follow a normal distribution. Now we will do the same for Math scores.\n\nad.test(exam$MATHS)\n\n\n    Anderson-Darling normality test\n\ndata:  exam$MATHS\nA = 7.9125, p-value &lt; 2.2e-16\n\n\nSince the p-value &lt; 0.05, we have enough statistical evidence to reject the null hypothesis and conclude that the Math scores also do not follow normal distribution.\nIn the code chunk below, ggscatterstats() is used to build a visual for Significant Test of Correlation between Maths scores and English scores.\n\nggscatterstats(\n  data = exam,\n  x = MATHS,\n  y = ENGLISH,\n  type='nonparametric', # 'parametric', 'robust', 'bayes'\n  marginal = FALSE,\n  )\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nI have chosen a non parametric version of this test as both Math and EL scores do not follow normal distribution.\n\n\n\n\n\nIn the code chunk below, the Maths scores is binned into a 4-class variable by using cut().\nWe will create a new dataframe exam1 similar to exam df but with extra column called ‘MATHS_bins’.\n\nexam1 &lt;- exam %&gt;% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0,60,75,85,100))\n)\n\n\nexam1\n\n# A tibble: 322 × 8\n   ID         CLASS GENDER RACE    ENGLISH MATHS SCIENCE MATHS_bins\n   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;     \n 1 Student321 3I    Male   Malay        21     9      15 (0,60]    \n 2 Student305 3I    Female Malay        24    22      16 (0,60]    \n 3 Student289 3H    Male   Chinese      26    16      16 (0,60]    \n 4 Student227 3F    Male   Chinese      27    77      31 (75,85]   \n 5 Student318 3I    Male   Malay        27    11      25 (0,60]    \n 6 Student306 3I    Female Malay        31    16      16 (0,60]    \n 7 Student313 3I    Male   Chinese      31    21      25 (0,60]    \n 8 Student316 3I    Male   Malay        31    18      27 (0,60]    \n 9 Student312 3I    Male   Malay        33    19      15 (0,60]    \n10 Student297 3H    Male   Indian       34    49      37 (0,60]    \n# ℹ 312 more rows\n\n\nIn this code chunk below ggbarstats() is used to build a visual for Significant Test of Association.\n(Two categorical variables) H0: There is no association between mathbin and gender.\nH1: There is an association between mathbin and gender.\n\nggbarstats(exam1,\n            x=MATHS_bins,\n            y=GENDER)\n\n\n\n\nFrom the results above , p-value &gt; 0.05 thus we have not enough statistical evidence to reject the null hypothesis that there is not association between the mathbin and gender variables."
  },
  {
    "objectID": "Proposal/Proposal.html#visualising-models",
    "href": "Proposal/Proposal.html#visualising-models",
    "title": "Here in Loompa land, it’s both luscious and green",
    "section": "",
    "text": "In this section, I will learn how to visualise model diagnostic and model parameters by using parameters package.\n\nToyota Corolla case study will be used. The purpose of study is to build a model to discover factors affecting prices of used-cars by taking into consideration a set of explanatory variables."
  },
  {
    "objectID": "Proposal/Proposal.html#installing-and-loading-the-required-libraries",
    "href": "Proposal/Proposal.html#installing-and-loading-the-required-libraries",
    "title": "Here in Loompa land, it’s both luscious and green",
    "section": "",
    "text": "pacman::p_load(readxl, performance, parameters, see)\n\n\n\nIn the code chunk below, read_xls() of readxl package is used to import the data worksheet of ToyotaCorolla.xls workbook into R.\n\ncar_resale &lt;- read_xls('data/ToyotaCorolla.xls',\n                       sheet='data')\n\n\nglimpse(car_resale)\n\nRows: 1,436\nColumns: 38\n$ Id               &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16…\n$ Model            &lt;chr&gt; \"TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\", \"TOY…\n$ Price            &lt;dbl&gt; 13500, 13750, 13950, 14950, 13750, 12950, 16900, 1860…\n$ Age_08_04        &lt;dbl&gt; 23, 23, 24, 26, 30, 32, 27, 30, 27, 23, 25, 22, 25, 3…\n$ Mfg_Month        &lt;dbl&gt; 10, 10, 9, 7, 3, 1, 6, 3, 6, 10, 8, 11, 8, 2, 1, 5, 3…\n$ Mfg_Year         &lt;dbl&gt; 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002,…\n$ KM               &lt;dbl&gt; 46986, 72937, 41711, 48000, 38500, 61000, 94612, 7588…\n$ Fuel_Type        &lt;chr&gt; \"Diesel\", \"Diesel\", \"Diesel\", \"Diesel\", \"Diesel\", \"Di…\n$ HP               &lt;dbl&gt; 90, 90, 90, 90, 90, 90, 90, 90, 192, 69, 192, 192, 19…\n$ Met_Color        &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,…\n$ Color            &lt;chr&gt; \"Blue\", \"Silver\", \"Blue\", \"Black\", \"Black\", \"White\", …\n$ Automatic        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ CC               &lt;dbl&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 1800,…\n$ Doors            &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,…\n$ Cylinders        &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,…\n$ Gears            &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 5, 5,…\n$ Quarterly_Tax    &lt;dbl&gt; 210, 210, 210, 210, 210, 210, 210, 210, 100, 185, 100…\n$ Weight           &lt;dbl&gt; 1165, 1165, 1165, 1165, 1170, 1170, 1245, 1245, 1185,…\n$ Mfr_Guarantee    &lt;dbl&gt; 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,…\n$ BOVAG_Guarantee  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,…\n$ Guarantee_Period &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 12, 3, 3, 3, 3, 3, 3, 1…\n$ ABS              &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airbag_1         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airbag_2         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,…\n$ Airco            &lt;dbl&gt; 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Automatic_airco  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,…\n$ Boardcomputer    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,…\n$ CD_Player        &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,…\n$ Central_Lock     &lt;dbl&gt; 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Powered_Windows  &lt;dbl&gt; 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Power_Steering   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Radio            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,…\n$ Mistlamps        &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,…\n$ Sport_Model      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,…\n$ Backseat_Divider &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,…\n$ Metallic_Rim     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,…\n$ Radio_cassette   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,…\n$ Tow_Bar          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,…\n\n\nNotice that the output object car_resale is a tibble data frame.\n\n\n\nThe code chunk below is used to calibrate a multiple linear regression model by using lm() of Base Stats of R.\n\nmodel &lt;- lm(Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period,\n            data=car_resale)\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\n\n\nIn the code chunk, check_collinearity() of performance package.\n\ncheck_collinearity(model)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\ncheck_c &lt;- check_collinearity(model)\nplot(check_c)\n\n\n\n\nWe can see high collinearity between Age and Mfg_Year. One is derived from the other. We should remove one of them and repeat muliti collinearity check again for the new model.\n\n\n\nIn the code chunk, check_normality() of performance package.\nNotice that the Mfg_Year variable has been removed from the independent variables list.\n\nmodel1 &lt;- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\n\ncheck_c1 &lt;- check_collinearity(model1)\nplot(check_c1)\n\n\n\n\n\ncheck_n &lt;- check_normality(model1)\n\n\nplot(check_n)\n\n\n\n\n\n\n\n\n\n\nRecap: Assumptions of linear regression\n\n\n\nIn linear regression, one of the key assumptions is that the residuals (the differences between the predicted values and the actual values) are normally distributed. The normality assumption is important because it affects the validity of statistical inference procedures such as hypothesis testing and confidence intervals.\nIf the residuals are not normally distributed, it may indicate that the linear regression model is not a good fit for the data and that alternative modeling approaches may be needed.\n\n\n\n\n\nIn the code chunk, check_heteroscedasticity() of performance package.\nHeteroscedasticity refers to a situation where the variance of the errors (or residuals) in the linear regression model is not constant across different levels of the predictor variable(s).\nIf heteroscedasticity is detected, there are several ways to address it, including transforming the data, using weighted least squares regression, or using robust standard errors. In DAl, we rebuild another model by creating subclasses out of the original Y variable.\n\ncheck_h &lt;- check_heteroscedasticity(model1)\n\n\nplot(check_h)\n\n\n\n\nFrom the graph above, there is a slight sign of heteroscedasticity as the residuals seem to be funnelled outwards as the fitted values increase.\n\n\n\nWe can also perform the complete check by using check_model().\n\ncheck_model(model1)\n\n\n\n\n\n\n\nIn the code below, plot() of see package and parameters() of parameters package is used to visualise the parameters of a regression model.\n\nplot(parameters(model1))\n\n\n\n\n\n\n\nIn the code below, ggcoefstats() of ggstatsplot package to visualise the parameters of a regression model.\n\nggcoefstats(model1, \n            output = \"plot\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that there are five types of callouts, including: note, warning, important, tip, and caution.\n\n\n\n\n\n\n\n\nTip with Title\n\n\n\nThis is an example of a callout with a title.\n\n\n\n\n\n\n\n\nExpand To Learn About Collapse\n\n\n\n\n\nThis is an example of a ‘folded’ caution callout that can be expanded by the user. You can use collapse=\"true\" to collapse it by default or collapse=\"false\" to make a collapsible callout that is expanded by default."
  }
]